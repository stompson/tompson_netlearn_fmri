{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Whole-Brain PPI Analysis\n",
    "This script takes the preprocessed brain activity from the network learning task, extracts timeseries of activation for each node in the Schaefer+Harvard-Oxford combined atlas, computes PPI scores for each node pair, and then repeats the PPI analysis 500 times after shuffling the trial order each time to generate null models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/steventompson/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2878: DeprecationWarning: Python2 support is deprecated and will be removed in a future release. Consider switching to Python3.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/Users/steventompson/anaconda2/lib/python2.7/site-packages/statsmodels/tsa/statespace/tools.py:59: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import (_representation, _kalman_filter, _kalman_smoother,\n",
      "/Users/steventompson/anaconda2/lib/python2.7/site-packages/statsmodels/tsa/statespace/tools.py:59: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  from . import (_representation, _kalman_filter, _kalman_smoother,\n",
      "/Users/steventompson/anaconda2/lib/python2.7/site-packages/statsmodels/tsa/kalmanf/kalmanfilter.py:33: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import kalman_loglike\n",
      "/Users/steventompson/anaconda2/lib/python2.7/site-packages/statsmodels/tsa/kalmanf/kalmanfilter.py:33: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  from . import kalman_loglike\n",
      "/Users/steventompson/anaconda2/lib/python2.7/site-packages/statsmodels/tsa/regime_switching/markov_switching.py:29: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from statsmodels.tsa.regime_switching._hamilton_filter import (\n",
      "/Users/steventompson/anaconda2/lib/python2.7/site-packages/statsmodels/tsa/regime_switching/markov_switching.py:29: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  from statsmodels.tsa.regime_switching._hamilton_filter import (\n",
      "/Users/steventompson/anaconda2/lib/python2.7/site-packages/statsmodels/nonparametric/kde.py:22: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .linbin import fast_linbin\n",
      "/Users/steventompson/anaconda2/lib/python2.7/site-packages/statsmodels/nonparametric/kde.py:22: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  from .linbin import fast_linbin\n",
      "/Users/steventompson/anaconda2/lib/python2.7/site-packages/statsmodels/nonparametric/smoothers_lowess.py:11: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._smoothers_lowess import lowess as _lowess\n",
      "/Users/steventompson/anaconda2/lib/python2.7/site-packages/statsmodels/nonparametric/smoothers_lowess.py:11: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  from ._smoothers_lowess import lowess as _lowess\n",
      "/Users/steventompson/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/base.py:35: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ..utils.seq_dataset import ArrayDataset, CSRDataset\n",
      "/Users/steventompson/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/base.py:35: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  from ..utils.seq_dataset import ArrayDataset, CSRDataset\n",
      "/Users/steventompson/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:23: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ..utils import arrayfuncs, as_float_array, check_X_y, deprecated\n",
      "/Users/steventompson/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:23: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  from ..utils import arrayfuncs, as_float_array, check_X_y, deprecated\n",
      "/Users/steventompson/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/coordinate_descent.py:29: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import cd_fast\n",
      "/Users/steventompson/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/coordinate_descent.py:29: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  from . import cd_fast\n",
      "/Users/steventompson/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/__init__.py:22: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .sgd_fast import Hinge, Log, ModifiedHuber, SquaredLoss, Huber\n",
      "/Users/steventompson/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/__init__.py:22: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  from .sgd_fast import Hinge, Log, ModifiedHuber, SquaredLoss, Huber\n",
      "/Users/steventompson/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/sag.py:12: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .sag_fast import sag\n",
      "/Users/steventompson/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/sag.py:12: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  from .sag_fast import sag\n",
      "/Users/steventompson/anaconda2/lib/python2.7/site-packages/sklearn/svm/base.py:8: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import libsvm, liblinear\n",
      "/Users/steventompson/anaconda2/lib/python2.7/site-packages/sklearn/svm/base.py:8: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  from . import libsvm, liblinear\n",
      "/Users/steventompson/anaconda2/lib/python2.7/site-packages/sklearn/utils/graph.py:15: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .graph_shortest_path import graph_shortest_path  # noqa\n",
      "/Users/steventompson/anaconda2/lib/python2.7/site-packages/sklearn/utils/graph.py:15: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  from .graph_shortest_path import graph_shortest_path  # noqa\n",
      "/Users/steventompson/anaconda2/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:28: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._online_lda import (mean_change, _dirichlet_expectation_1d,\n",
      "/Users/steventompson/anaconda2/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:28: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  from ._online_lda import (mean_change, _dirichlet_expectation_1d,\n",
      "/Users/steventompson/anaconda2/lib/python2.7/site-packages/sklearn/isotonic.py:12: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._isotonic import _inplace_contiguous_isotonic_regression, _make_unique\n",
      "/Users/steventompson/anaconda2/lib/python2.7/site-packages/sklearn/isotonic.py:12: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  from ._isotonic import _inplace_contiguous_isotonic_regression, _make_unique\n",
      "/Users/steventompson/anaconda2/lib/python2.7/site-packages/sklearn/manifold/t_sne.py:24: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import _utils\n",
      "/Users/steventompson/anaconda2/lib/python2.7/site-packages/sklearn/manifold/t_sne.py:24: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  from . import _utils\n",
      "/Users/steventompson/anaconda2/lib/python2.7/site-packages/sklearn/tree/tree.py:40: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._criterion import Criterion\n",
      "/Users/steventompson/anaconda2/lib/python2.7/site-packages/sklearn/tree/tree.py:40: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  from ._criterion import Criterion\n",
      "/Users/steventompson/anaconda2/lib/python2.7/site-packages/sklearn/cluster/k_means_.py:35: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import _k_means\n",
      "/Users/steventompson/anaconda2/lib/python2.7/site-packages/sklearn/cluster/k_means_.py:35: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  from . import _k_means\n",
      "/Users/steventompson/anaconda2/lib/python2.7/site-packages/sklearn/cluster/hierarchical.py:23: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import _hierarchical\n",
      "/Users/steventompson/anaconda2/lib/python2.7/site-packages/sklearn/cluster/hierarchical.py:23: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  from . import _hierarchical\n",
      "/Users/steventompson/anaconda2/lib/python2.7/site-packages/sklearn/cluster/dbscan_.py:19: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._dbscan_inner import dbscan_inner\n",
      "/Users/steventompson/anaconda2/lib/python2.7/site-packages/sklearn/cluster/dbscan_.py:19: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  from ._dbscan_inner import dbscan_inner\n",
      "/Users/steventompson/anaconda2/lib/python2.7/site-packages/nipy/algorithms/statistics/__init__.py:9: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import intvol, rft, onesample, formula\n",
      "/Users/steventompson/anaconda2/lib/python2.7/site-packages/nipy/algorithms/statistics/__init__.py:9: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 176\n",
      "  from . import intvol, rft, onesample, formula\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Importing Packages\n",
      "nilearn: 0.4.2\n",
      "nipy: 0.4.2\n",
      "nistats: 0.0.1b\n",
      "numpy: 1.16.4\n",
      "pandas: 0.23.0\n",
      "scipy: 1.1.0\n",
      "statsmodels: 0.9.0\n"
     ]
    }
   ],
   "source": [
    "#importing libraries\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import os, sys\n",
    "import nilearn, nipy, nistats, scipy, statsmodels\n",
    "from os.path import join as opj\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from nilearn import image, plotting, input_data\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "\n",
    "from nipy.modalities.fmri import hrf\n",
    "from nipy.modalities.fmri import hemodynamic_models\n",
    "\n",
    "from nistats.design_matrix import make_first_level_design_matrix\n",
    "\n",
    "print('Done Importing Packages')\n",
    "\n",
    "print('nilearn:',nilearn.__version__)\n",
    "print('nipy:',nipy.__version__)\n",
    "print('nistats:',nistats.__version__)\n",
    "print('numpy:',np.__version__)\n",
    "print('pandas:',pd.__version__)\n",
    "print('scipy:',scipy.__version__)\n",
    "print('statsmodels:',statsmodels.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create miscellaneous functions that you will use throughout script\n",
    "\n",
    "def convert_cfg_vec_to_adj_matr(conn_vec):\n",
    "    '''\n",
    "    Convert connections to adjacency matrix\n",
    "    Assumes symmetric connectivity\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        conn_vec: numpy.ndarray\n",
    "            Vector with shape (n_conn,) specifying unique connections\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        adj_matr: numpy.ndarray\n",
    "            Symmetric matrix with shape (n_node, n_node)\n",
    "    '''\n",
    "    # Standard param checks\n",
    "    #errors.check_type(conn_vec, np.ndarray)\n",
    "    if not len(conn_vec.shape) == 1:\n",
    "        raise ValueError('%r has more than 1-dimension')\n",
    "\n",
    "    # Compute number of nodes\n",
    "    n_node = int(np.floor(np.sqrt(2*len(conn_vec)))+1)\n",
    "\n",
    "    # Compute upper triangle indices (by convention)\n",
    "    triu_ix, triu_iy = np.triu_indices(n_node, k=1)\n",
    "\n",
    "    # Convert to adjacency matrix\n",
    "    adj_matr = np.zeros((n_node, n_node))\n",
    "    adj_matr[triu_ix, triu_iy] = conn_vec\n",
    "\n",
    "    adj_matr += adj_matr.T\n",
    "\n",
    "    return adj_matr\n",
    "\n",
    "\n",
    "def convert_adj_matr_to_cfg_matr(adj_matr):\n",
    "    '''\n",
    "    Convert connections to adjacency matrix\n",
    "    Assumes symmetric connectivity\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        adj_matr: numpy.ndarray\n",
    "            Matrix with shape (n_win, n_node, n_node)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        cfg_matr: numpy.ndarray\n",
    "            Symmetric matrix with shape (n_win, n_conn)\n",
    "    '''\n",
    "    # Standard param checks\n",
    "    #errors.check_type(adj_matr, np.ndarray)\n",
    "    if not len(adj_matr.shape) == 3:\n",
    "        raise ValueError('%r requires 3-dimensions (n_win, n_node, n_node)')\n",
    "\n",
    "    # Compute number of nodes\n",
    "    n_node = adj_matr.shape[1]\n",
    "\n",
    "    # Compute upper triangle indices (by convention)\n",
    "    triu_ix, triu_iy = np.triu_indices(n_node, k=1)\n",
    "\n",
    "    # Convert to configuration matrix\n",
    "    cfg_matr = adj_matr[:, triu_ix, triu_iy]\n",
    "\n",
    "    return cfg_matr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set paths and subject list\n",
    "Then we define the paths to the subjects we want to extract timeseries for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to directory where you saved the data\n",
    "home_path = '/Users/steventompson/Git/tompson_netlearn_fmri'\n",
    "\n",
    "data_dir = opj(home_path,'data')\n",
    "path_CodeDir = opj(home_path,'scripts')\n",
    "funcpath = opj(data_dir,'subjs')\n",
    "\n",
    "\n",
    "#path_InpData = opj(data_dir,'subjs')\n",
    "path_InpData = '/Volumes/Tompson_fMRIData2/SNL_Study/data/'\n",
    "path_OutpData1 = opj(data_dir,'timeSeries','netLearn_cfr_schaefer')\n",
    "path_OutpData2 = opj(data_dir,'timeSeries','deconv_temp')\n",
    "path_OutpData3 = opj(data_dir,'timeSeries','netLearn_cfr_schaefer_deconv')\n",
    "path_OutpData4 = opj(data_dir,'netLearn_ppi')\n",
    "path_OutpData5 = opj(data_dir,'netLearn_ppi_null')\n",
    "\n",
    "\n",
    "\n",
    "#Check if paths exist and, if they don't, create them\n",
    "for path in [path_InpData, path_OutpData1, path_OutpData2, \n",
    "             path_OutpData3, path_OutpData4, path_OutpData5]:\n",
    "    if not os.path.exists(path):\n",
    "        print('\\nPath: {}, does not exist'.format(path))\n",
    "        os.makedirs(path)\n",
    "\n",
    "print('Set data paths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjs=sorted([s for s in os.listdir(path_InpData) if 'SNL' in s])\n",
    "\n",
    "bad_subjs = ['SNL_001','SNL_004','SNL_028']\n",
    "subjs = [s for s in subjs if s not in bad_subjs]\n",
    "#subjs.reverse()\n",
    "n_subjs = len(subjs)\n",
    "print('We have %d subjects' % (n_subjs))\n",
    "\n",
    "#Create list of run IDs\n",
    "runs=['run1','run2','run3','run4','run5','run6','run7','run8','run9','run10']\n",
    "\n",
    "param_list=[]\n",
    "for ii in subjs:\n",
    "    for jj in runs:\n",
    "        param_list.append([ii,jj])\n",
    "\n",
    "fd_thresh = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Extract brain activity\n",
    "Use nilearn to extract the timeseries for each node and clean the data to control for confounds. Most of the cleaning of the data was done at the preprocessing step, but we will also regress out high-variance-confounds (similar to compcorr) and standardize the timeseries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define functions\n",
    "#Define functions that will extract/create the confound regressors, \n",
    "#extract and clean the time-series for each node, \n",
    "#and calculate the framewise displacement.\n",
    "\n",
    "def clean_time_series(subjID, runID, atlas_path, hv_confounds=None, fd_thresh = None):  \n",
    "    '''\n",
    "    Need to add description\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        varname: var type\n",
    "            Description\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        varname: var type\n",
    "            Description\n",
    "    '''\n",
    "    funcPath=os.path.join(path_InpData,subjID,'func')\n",
    "    anatPath=os.path.join(path_InpData,subjID,'anat')\n",
    "    nifti_file = 'norm_rg_bp_dmt_stc_mc_dsp_BOLD_{}.nii.gz'.format(runID)\n",
    "    conf_filename = opj(funcPath,'bp_dmt_stc_mc_dsp_BOLD_{}.nii.gz_36Params.1D'.format(runID))\n",
    "    func_filename = opj(funcPath, nifti_file)  #preprocessed data\n",
    "\n",
    "    #Extract high variance confounds\n",
    "    print('Extracting high variance confounds')\n",
    "    hv_confounds = nilearn.image.high_variance_confounds(func_filename) #high variance confounds\n",
    "    \n",
    "    print('Setting up mask')\n",
    "    masker = input_data.NiftiLabelsMasker(atlas_path,\n",
    "                                           detrend=False, \n",
    "                                           standardize=True, \n",
    "                                           low_pass=None, high_pass=None, \n",
    "                                           t_r=1,  \n",
    "                                           memory='nilearn_cache', memory_level=1); #verbose=2 by default nothing should be printed\n",
    "      \n",
    "    print('Extracting cleaned data')\n",
    "    cleaned_time_series = masker.fit_transform(func_filename,confounds=[hv_confounds])  \n",
    "    \n",
    "    print('Computing framewise displacement')\n",
    "    if fd_thresh>0:     \n",
    "        FD = compute_fd(conf_filename)\n",
    "        bad_fd_vols  = np.where(FD > fd_thresh, 1,0)\n",
    "    else:\n",
    "        bad_fd_vols = []\n",
    "    \n",
    "    print('Percent of volumes with excessive head motion: {}'.format(np.mean(bad_fd_vols)))\n",
    "        \n",
    "    return cleaned_time_series,  FD, bad_fd_vols\n",
    "\n",
    "def compute_fd(conf_filename):\n",
    "    '''\n",
    "    Load motion parameters file and compute framewise displacement. \n",
    "    This function is based on Powers' 2012 as well as Bramila tools and \n",
    "    Poldrack's fMRI QA script. Important note: It assumes the input is a \n",
    "    SPM-realignment parameter file. FSL uses a different ordering and \n",
    "    thus cannot be used blindly... Beware!\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        conf_filename: character string\n",
    "            String indicating the name of the motion confound file to load\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        FD: numpy nd.array\n",
    "            1-D array with length of the number of volumes in the task run, \n",
    "            representing the framewise displacement for each volume.\n",
    "            \n",
    "    '''\n",
    "\n",
    "\n",
    "    motpars = np.loadtxt(conf_filename)[:,range(6)]\n",
    "\n",
    "    # compute absolute displacement\n",
    "    dmotpars=np.zeros(motpars.shape)\n",
    "    \n",
    "    dmotpars[1:,:]=np.abs(motpars[1:,:] - motpars[:-1,:])\n",
    "    \n",
    "    # convert rotation to displacement on a 50 mm sphere\n",
    "    # mcflirt returns rotation in radians\n",
    "    # from Jonathan Power:\n",
    "    # The conversion is simple - you just want the length of an arc that a rotational\n",
    "    # displacement causes at some radius. Circumference is pi*diameter, and we used a 50 mm radius. \n",
    "    # Multiply that circumference by (degrees/360) or (radians/2*pi) to get the \n",
    "    # length of the arc produced by a rotation.  \n",
    "    headradius=50\n",
    "    disp=dmotpars.copy()\n",
    "    disp[:,0:3]=np.pi*headradius*2*(disp[:,0:3]/(2*np.pi))\n",
    "    \n",
    "    FD=np.sum(disp,1)\n",
    "      \n",
    "    return FD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import brain atlas\n",
    "Next, import the Schaefer+Subcortical atlas with 400 cortical regions (200 per hemisphere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_dir='/data/jux/stompson/tools/BrainAtlases'\n",
    "maskPath='{}/Schaefer2018/schaefer400_harvard_oxford_2mm_mni_17network.nii.gz'.format(template_dir)\n",
    "smask=nb.load(maskPath)\n",
    "sdata=smask.get_data()\n",
    "\n",
    "atlas_file='{}/neurosynth_data/s400ho_ns_netLearn_2mm.pickle'.format(template_dir)\n",
    "atlas_info=pd.read_pickle(atlas_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract data from individual subjects, clean data, and save time_series\n",
    "Here we extract the data from individual subjects during each run of the network learning task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runNum = 0\n",
    "\n",
    "for n in range(len(param_list)): \n",
    "    #Set the subject and run IDs based on the parameter list generated above\n",
    "    subjID=param_list[n][0]\n",
    "    runID=param_list[n][1]\n",
    "    print('Working on param # {}: subject {}, {}'.format(n+1,param_list[n][0],param_list[n][1]))\n",
    "    print('...')\n",
    "    \n",
    "    #Set paths for important files/folders\n",
    "    anatPath=opj(path_InpData,subjID,'anat')\n",
    "    funcPath=os.path.join(path_InpData,subjID,'func')\n",
    "    conf_filename = opj(funcPath,'bp_dmt_stc_mc_dsp_BOLD_{}.nii.gz_36Params.1D'.format(runID)) #motion confounds\n",
    "    saveFile1=os.path.join(path_OutpData1, ('{}_netLearn_{}_TS_Schaefer400+Subcort'.format(subjID,runID)))\n",
    "    \n",
    "\n",
    "    #compute cleaned time-series controlling for high-variance confounds and computing fd and bad_fd_vols as well\n",
    "    cleaned_time_series, fds, bad_fd_vols  = clean_time_series(subjID, runID, \n",
    "                                                               atlas_path=maskPath, fd_thresh = fd_thresh)\n",
    "    \n",
    "    #save important data for each subject/run to a new numpy zipped file\n",
    "    np.savez(saveFile1, cleaned_time_series, fds, bad_fd_vols)\n",
    "    print('Saved file: {}.npz'.format(saveFile1))\n",
    "    print('...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Deconvolve timeseries\n",
    "\n",
    "Take the timeseries for each node extracted in the first step and deconvolve it from the hemodynamic response function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "def load_sub_data1(subjID,runID):\n",
    "    '''\n",
    "    Need to add description\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        varname: var type\n",
    "            Description\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        varname: var type\n",
    "            Description\n",
    "    '''\n",
    "    #subjID and runID are strings to pass to filename\n",
    "    #file is an npz file with an n_trs x n_nodes matrix (so it is a 2D matrix with stacked timeseries)\n",
    "    loadFile1=opj(path_OutpData1, ('{}_netLearn_{}_TS_Schaefer400+Subcort.npz'.format(subjID,runID)))\n",
    "    data=np.load(loadFile1)\n",
    "    timeseries=data['arr_0']\n",
    "    return timeseries\n",
    "\n",
    "def deconvolve_subj_data(paramID):\n",
    "    '''\n",
    "    Need to add description\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        varname: var type\n",
    "            Description\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        varname: var type\n",
    "            Description\n",
    "    '''\n",
    "    #paramID is the index for which subject and run ids to use for this task\n",
    "    print('Working on param # {}: subject {}, {}'.format(paramID+1,param_list[paramID][0],param_list[paramID][1]))\n",
    "    subjID=param_list[paramID][0]\n",
    "    runID=param_list[paramID][1]\n",
    "    saveFile='{}/{}_netLearn_{}_deconvolved_ts'.format(path_OutpData3,subjID,runID)\n",
    "\n",
    "    #Load subject timeseries data\n",
    "    bold_ts=load_sub_data1(subjID,runID)\n",
    "    n_trs=bold_ts.shape[0]\n",
    "    n_nodes=bold_ts.shape[1]\n",
    "\n",
    "    #Create empty dataframe to fill with deconvolved data\n",
    "    index=range(n_trs)\n",
    "    columns=range(n_nodes)\n",
    "    dcv_ts=pd.DataFrame(index=index,columns=columns)\n",
    "\n",
    "    cwd=os.getcwd()\n",
    "    os.chdir(path_OutpData2)\n",
    "    \n",
    "    #Loop over nodes and deconvolve each timeseries\n",
    "    for n,node_ts in enumerate(bold_ts.T):\n",
    "        if n==0:\n",
    "            print('Working on node #{}'.format(n),end='')\n",
    "        elif n%50==0:\n",
    "            print(n,end='')\n",
    "        elif n%10==0:\n",
    "            print('.'.format(n),end='')\n",
    "\n",
    "        #Save node timeseries to 1D file\n",
    "        sFile1='{}/{}_{}_bold_ts_node{}.1D'.format(path_OutpData2,subjID,runID,n)\n",
    "        pd.DataFrame(node_ts).to_csv(sFile1,index=False,header=False)\n",
    "\n",
    "        #Deconvolve BOLD signal for node using AFNI's 3dTfitter function\n",
    "        sFile2='{}/{}_{}_neur_ts_node{}'.format(path_OutpData2,subjID,runID,n)\n",
    "        cmd='3dTfitter -RHS {} -FALTUNG gloverHRF.1D {} 012 0'.format(sFile1,sFile2)\n",
    "        subprocess.call(cmd,shell=True)\n",
    "\n",
    "        #3dTfitter saves the file as a .1D text file, so need to reload that and add to \n",
    "        #deconvolved timeseries dataframe\n",
    "        node_dc_ts=np.loadtxt('{}.1D'.format(sFile2)).tolist()\n",
    "        dcv_ts.loc[:,n]=node_dc_ts\n",
    "\n",
    "    os.chdir(cwd)\n",
    "\n",
    "    #save important data for each subject/run to a new numpy zipped file\n",
    "    np.savez(saveFile, np.array(dcv_ts))\n",
    "    print('...')\n",
    "    print('Saved file: {}.npz'.format(saveFile))\n",
    "    print('...')\n",
    "\n",
    "\n",
    "for n in range(len(param_list)): \n",
    "    #Set the subject and run IDs based on the parameter list generated above\n",
    "    subjID=param_list[n][0]\n",
    "    runID=param_list[n][1]\n",
    "    print('Working on param # {}: subject {}, {}'.format(n+1,param_list[n][0],param_list[n][1]))\n",
    "    print('...')\n",
    "    deconvolve_subj_data(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4: Get PPI matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set task variables and trial-level data for each subject and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load master file with subject IDs and trial info as well as list of subject IDs for scanner      \n",
    "masterFile=pd.read_csv('{}/subj_data/netLearn_masterFile_27subjs.csv'.format(data_dir))\n",
    "subj_links=pd.read_csv('{}/subj_data/netLearn_IDs_26subjs.csv'.format(data_dir))\n",
    "\n",
    "# # Set task data\n",
    "\n",
    "#Create run labels\n",
    "masterFile['Run']=0\n",
    "masterFile.loc[np.in1d(np.array(masterFile['trialNum']),range(200)),'Run']=1\n",
    "masterFile.loc[np.in1d(np.array(masterFile['trialNum']),range(200,400)),'Run']=2\n",
    "masterFile.loc[np.in1d(np.array(masterFile['trialNum']),range(400,600)),'Run']=3\n",
    "masterFile.loc[np.in1d(np.array(masterFile['trialNum']),range(600,800)),'Run']=4\n",
    "masterFile.loc[np.in1d(np.array(masterFile['trialNum']),range(800,1000)),'Run']=5\n",
    "\n",
    "masterFile.loc[masterFile['transition']==0,'transition']='x'\n",
    "masterFile.loc[masterFile['transition']==1,'transition']='transition'\n",
    "\n",
    "masterFile=masterFile.loc[masterFile['transition']=='transition',:]\n",
    "\n",
    "#Set experiment parameters\n",
    "tr = 1.0\n",
    "condlist=['transition']\n",
    "hmlist=['tx','ty','tz','rx','ry','rz']\n",
    "covarlist=['tx','ty','tz','rx','ry','rz','constant']\n",
    "n_nodes=410"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sub_data2(subjID,runID):\n",
    "    '''\n",
    "    Need to add description\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        varname: var type\n",
    "            Description\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        varname: var type\n",
    "            Description\n",
    "    '''\n",
    "    #subjID and runID are strings to pass to the filename to load\n",
    "    #load deconvolved timeseries matrix with n_trs x n_nodes shape\n",
    "    loadFile1='{}/{}_netLearn_{}_deconvolved_ts.npz'.format(path_OutpData3,subjID,runID)\n",
    "    data=np.load(loadFile1)\n",
    "    timeseries=data['arr_0']\n",
    "    #for x in range(timeseries.shape[1]):\n",
    "    #    timeseries[:,x]=ss.zscore(timeseries[:,x])\n",
    "    \n",
    "    #Load head motion parameters file\n",
    "    conf_filename = opj(funcPath,'bp_dmt_stc_mc_dsp_BOLD_{}.nii.gz_36Params.1D'.format(runID))  #motion confounds\n",
    "    motpars = pd.read_csv(conf_filename, header=None, delim_whitespace=True)\n",
    "    motpars = motpars.loc[:,:5].as_matrix()\n",
    "    \n",
    "    #Z-score the head motion parameters (step is probably not necessary but yielded\n",
    "    #beta weights that were standardized and thus more interpretable)\n",
    "    for x in range(motpars.shape[1]):\n",
    "        motpars[:,x]=ss.zscore(motpars[:,x])\n",
    "    return timeseries,motpars\n",
    "\n",
    "def create_sub_design(subjID,runID,df,regs,regnames):\n",
    "    '''\n",
    "    Need to add description\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        varname: var type\n",
    "            Description\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        varname: var type\n",
    "            Description\n",
    "    '''\n",
    "    #Note: ppt in Condition #1 saw non-social task first, whereas ppt in Condition #2 saw social task first\n",
    "    #Convert subject and run ids to integers\n",
    "    pID=int(subjID[-3:])\n",
    "    rID=int(runID.replace('run',''))\n",
    "    n_scans = len(regs)\n",
    "    frame_times = np.arange(n_scans) * tr\n",
    "    subdata=df.loc[df['pID']==pID,:]\n",
    "    #Choose subject run based on counterbalance condition number\n",
    "    #Can simplify this code if you didn't counterbalance tasks (or only have one task)\n",
    "    if int(subj_links.loc[subj_links['scanID']==subjID,'CondNum'])==1 and rID<6:\n",
    "        subdata=subdata.loc[subdata['Cond']=='NS',:]\n",
    "        subdata=subdata.loc[subdata['Run']==rID,:]\n",
    "    elif int(subj_links.loc[subj_links['scanID']==subjID,'CondNum'])==1 and rID>5:\n",
    "        subdata=subdata.loc[subdata['Cond']=='Soc',:]\n",
    "        subdata=subdata.loc[subdata['Run']==rID-5,:]\n",
    "    elif int(subj_links.loc[subj_links['scanID']==subjID,'CondNum'])==2 and rID<6:\n",
    "        subdata=subdata.loc[subdata['Cond']=='Soc',:]\n",
    "        subdata=subdata.loc[subdata['Run']==rID,:]\n",
    "    elif int(subj_links.loc[subj_links['scanID']==subjID,'CondNum'])==2 and rID>5:\n",
    "        subdata=subdata.loc[subdata['Cond']=='NS',:]\n",
    "        subdata=subdata.loc[subdata['Run']==rID-5,:]\n",
    "    trials=subdata['transition'].tolist()\n",
    "    onsets=subdata['onset_raw'].tolist()\n",
    "    durs=[1.5]*len(trials)\n",
    "    \n",
    "    #Make design matrix using nistats\n",
    "    paradigm = pd.DataFrame({'trial_type': trials, 'onset': onsets,\n",
    "                             'duration': durs})\n",
    "    X1 = make_first_level_design_matrix(frame_times, paradigm, drift_model='polynomial',\n",
    "                                        add_regs=regs, add_reg_names=regnames, \n",
    "                                        drift_order=3, hrf_model=None)\n",
    "    return X1\n",
    "\n",
    "def create_ppi_design(df,roi,condlist,covarlist):\n",
    "    '''\n",
    "    Need to add description\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        varname: var type\n",
    "            Description\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        varname: var type\n",
    "            Description\n",
    "    '''\n",
    "    X2 = df.copy()\n",
    "    X2['ROI']=roi\n",
    "    cols=['ROI']\n",
    "    #Construct ppi terms\n",
    "    for cond in condlist:\n",
    "        #X2[str(cond+'_PPI')]=ss.zscore(np.multiply(X2['ROI'],X2[cond]))\n",
    "        X2[str(cond+'_PPI')]=np.multiply(X2['ROI'],X2[cond])\n",
    "        cols.append(cond)\n",
    "        cols.append(str(cond+'_PPI'))\n",
    "    cols.extend(covarlist)\n",
    "    return X2,cols\n",
    "\n",
    "def get_ppi_beta(df,dv,index):\n",
    "    '''\n",
    "    Need to add description\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        varname: var type\n",
    "            Description\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        varname: var type\n",
    "            Description\n",
    "    '''\n",
    "    #extract the beta weight for the ppi term (or any other terms) using the ppi design matrix\n",
    "    #and a timeseries for another node\n",
    "    #\n",
    "    X = df\n",
    "    y = dv\n",
    "    model = sm.OLS(y,X,hasconst=True).fit()\n",
    "    #predictions = model.predict(X)\n",
    "    #print(model.summary())\n",
    "    bweight=model.params[index]\n",
    "    return bweight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppivar = 'transition_PPI'\n",
    "\n",
    "for n in range(len(param_list)): \n",
    "    #Set the subject and run IDs based on the parameter list generated above\n",
    "    subjID=param_list[n][0]\n",
    "    runID=param_list[n][1]\n",
    "    print('Working on param # {}: subject {}, {}'.format(n+1,param_list[n][0],param_list[n][1]))\n",
    "    print('...')\n",
    "\n",
    "    #Set path and filename to save the ppi matrix\n",
    "    saveFile1=os.path.join(path_OutpData4, ('{}_netLearn_{}_{}_410nodes'.format(subjID,runID,ppivar)))\n",
    "\n",
    "    #Load subject trial and head motion data and create design matrix\n",
    "    data1,hm1=load_sub_data2(subjID,runID)\n",
    "    newdf=create_sub_design(subjID,runID,masterFile,hm1,hmlist)\n",
    "    #n_nodes=data1.shape[1]\n",
    "    ppiMat=np.zeros((n_nodes,n_nodes))\n",
    "\n",
    "    #Can set this to be a subset of the conditions by changing condlist above\n",
    "    newcondlist=[x for x in condlist if x in np.array(newdf.columns)]\n",
    "\n",
    "    #Loop over each node to create a design matrix for each node\n",
    "    for ii in range(n_nodes):\n",
    "        if ii==0:\n",
    "            print('Working on node {}'.format(ii),end='')\n",
    "        elif ii%50==0:\n",
    "            print(ii,end='')\n",
    "        elif ii%10==0:\n",
    "            print('.',end='')\n",
    "\n",
    "        ppidf,ppicols=create_ppi_design(newdf,data1[:,ii],newcondlist,covarlist)\n",
    "        ppidf=ppidf[ppicols]\n",
    "        hkernel = [hemodynamic_models.glover_hrf(tr, oversampling=1)]\n",
    "\n",
    "        #Z-score the design matrix so the beta weights are standardized (again can skip this step if you want)\n",
    "        for x in ppicols[:-len(covarlist)]:\n",
    "            ppidf.loc[:,x]=ss.zscore(np.convolve(ppidf.loc[:,x],hkernel[0])[:np.array(ppidf.loc[:,x]).size])\n",
    "\n",
    "        #Get the z-scored timeseries for each deconvolved timeseries to compute ppi for each node pair\n",
    "        for jj in range(n_nodes):\n",
    "            if jj!=ii:\n",
    "                conv_dv=ss.zscore(np.convolve(data1[:,jj],hkernel[0])[:np.array(data1[:,jj]).size])\n",
    "                betaweight=get_ppi_beta(ppidf,conv_dv,ppivar)\n",
    "                ppiMat[ii,jj]=betaweight\n",
    "\n",
    "    # Convert to cfg_vec to symmetrize the upper and lower triangles \n",
    "    # since ppi of y to x is not identical to x to y\n",
    "\n",
    "    # Compute upper triangle indices (by convention)\n",
    "    triu_ix, triu_iy = np.triu_indices(n_nodes, k=1)\n",
    "\n",
    "    # Convert to configuration matrix\n",
    "    ppivec_upp = ppiMat[triu_ix, triu_iy]\n",
    "    ppivec_low = ppiMat[triu_iy, triu_ix]\n",
    "\n",
    "    # Compute mean cfg_vec\n",
    "    ppivec=np.mean(np.vstack((ppivec_upp,ppivec_low)),axis=0)\n",
    "\n",
    "    # Convert back to adjacency matrix\n",
    "    ppiMat2=convert_cfg_vec_to_adj_matr(ppivec)\n",
    "\n",
    "    #save important data for each subject/run to a new numpy zipped file\n",
    "    np.savez(saveFile1, ppiMat=ppiMat2)\n",
    "    print('')\n",
    "    print('Saved file: {}.npz'.format(saveFile1))\n",
    "    print('...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5: Create null model PPI matrices\n",
    "Shuffle the trial order for each run and re-run the PPI analysis 500 times to generate null models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_null_sub_design(subjID,runID,df,regs,regnames):\n",
    "    '''\n",
    "    Need to add description\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        varname: var type\n",
    "            Description\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        varname: var type\n",
    "            Description\n",
    "    '''\n",
    "    #Note: ppt in Condition #1 saw non-social task first, whereas ppt in Condition #2 saw social task first\n",
    "    pID=int(subjID[-3:])\n",
    "    rID=int(runID.replace('run',''))\n",
    "    n_scans = len(regs)\n",
    "    frame_times = np.arange(n_scans) * tr\n",
    "    subdata=df.loc[df['pID']==pID,:]\n",
    "    if int(subj_links.loc[subj_links['scanID']==subjID,'CondNum'])==1 and rID<6:\n",
    "        subdata=subdata.loc[subdata['Cond']=='NS',:]\n",
    "        subdata=subdata.loc[subdata['Run']==rID,:]\n",
    "    elif int(subj_links.loc[subj_links['scanID']==subjID,'CondNum'])==1 and rID>5:\n",
    "        subdata=subdata.loc[subdata['Cond']=='Soc',:]\n",
    "        subdata=subdata.loc[subdata['Run']==rID-5,:]\n",
    "    elif int(subj_links.loc[subj_links['scanID']==subjID,'CondNum'])==2 and rID<6:\n",
    "        subdata=subdata.loc[subdata['Cond']=='Soc',:]\n",
    "        subdata=subdata.loc[subdata['Run']==rID,:]\n",
    "    elif int(subj_links.loc[subj_links['scanID']==subjID,'CondNum'])==2 and rID>5:\n",
    "        subdata=subdata.loc[subdata['Cond']=='NS',:]\n",
    "        subdata=subdata.loc[subdata['Run']==rID-5,:]\n",
    "    trials=subdata['transition'].tolist()\n",
    "    #onsets=subdata['onset_raw'].tolist()\n",
    "    durs=[trialdur]*len(trials)\n",
    "    \n",
    "    #randomize trial onsets\n",
    "    onset_sample=np.arange(0,(n_scans-10),trialdur)\n",
    "    onsets=np.sort(np.random.choice(onset_sample,size=len(trials),replace=False)).tolist()\n",
    "    \n",
    "    paradigm = pd.DataFrame({'trial_type': trials, 'onset': onsets,\n",
    "                             'duration': durs})\n",
    "    X1 = make_design_matrix(frame_times, paradigm, drift_model='polynomial',\n",
    "                        add_regs=regs, add_reg_names=regnames, \n",
    "                        drift_order=3, hrf_model=None)\n",
    "    return X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppivar = 'transition_PPI'\n",
    "\n",
    "for n in range(len(param_list)): \n",
    "    #Set the subject and run IDs based on the parameter list generated above\n",
    "    subjID=param_list[n][0]\n",
    "    runID=param_list[n][1]\n",
    "    print('Working on param # {}: subject {}, {}'.format(n+1,param_list[n][0],param_list[n][1]))\n",
    "    print('...')\n",
    "\n",
    "    data1,hm1=load_sub_data(subjID,runID)\n",
    "\n",
    "    # Set path and filename to save the null model ppi matrix\n",
    "    saveFile1=opj(path_OutpData5, ('{}_netLearn_{}_{}_410nodes_nulldist'.format(subjID,runID,ppivar)))\n",
    "\n",
    "    ppi_null=np.zeros((n_perm,n_nodes,n_nodes))\n",
    "    for perm_val in range(n_perm):\n",
    "        print('')\n",
    "        print('Working on permutation #{}'.format(perm_val+1))\n",
    "        newdf=create_null_sub_design(subjID,runID,masterFile,hm1,hmlist)\n",
    "        #n_nodes=data1.shape[1]\n",
    "        ppiMat=np.zeros((n_nodes,n_nodes))\n",
    "        newcondlist=[x for x in condlist if x in np.array(newdf.columns)]\n",
    "        for ii in range(n_nodes):\n",
    "            if ii==0:\n",
    "                print('Working on node {}'.format(ii),end='')\n",
    "            elif ii%50==0:\n",
    "                print(ii,end='')\n",
    "            elif ii%10==0:\n",
    "                print('.',end='')\n",
    "\n",
    "            ppidf,ppicols=create_ppi_design(newdf,data1[:,ii],newcondlist,covarlist)\n",
    "            ppidf=ppidf[ppicols]\n",
    "            hkernel = [hemodynamic_models.glover_hrf(tr, oversampling=1)]\n",
    "            for x in ppicols[:-len(covarlist)]:\n",
    "                ppidf.loc[:,x]=ss.zscore(np.convolve(ppidf.loc[:,x],hkernel[0])[:np.array(ppidf.loc[:,x]).size])\n",
    "            for jj in range(n_nodes):\n",
    "                if jj>ii:\n",
    "                    conv_dv=ss.zscore(np.convolve(data1[:,jj],hkernel[0])[:np.array(data1[:,jj]).size])\n",
    "                    betaweight=get_ppi_beta(ppidf,conv_dv,ppivar)\n",
    "                    ppiMat[ii,jj]=betaweight\n",
    "\n",
    "        # Convert to cfg_vec to symmetrize the upper and lower triangles\n",
    "\n",
    "        # Compute upper triangle indices (by convention)\n",
    "        triu_ix, triu_iy = np.triu_indices(n_nodes, k=1)\n",
    "        #tril_ix, tril_iy = np.tril_indices(n_nodes, k=-1)\n",
    "\n",
    "        # Convert to configuration matrix\n",
    "        ppivec_upp = ppiMat[triu_ix, triu_iy]\n",
    "        ppivec_low = ppiMat[triu_iy, triu_ix]\n",
    "\n",
    "        # Compute mean cfg_vec\n",
    "        ppivec=np.mean(np.vstack((ppivec_upp,ppivec_low)),axis=0)\n",
    "\n",
    "        # Convert back to adjacency matrix\n",
    "        ppiMat2=convert_cfg_vec_to_adj_matr(ppivec)\n",
    "\n",
    "        #Add to ppi_null array\n",
    "        ppi_null[perm_val,:,:]=ppiMat2\n",
    "\n",
    "    #save important data for each subject/run to a new numpy zipped file\n",
    "    np.savez(saveFile1, ppiMat=ppi_null)\n",
    "    print('')\n",
    "    print('Saved file: {}.npz'.format(saveFile1))\n",
    "    print('...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6: Combine run data for each subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ppiMat=matrix of ppi values for each node pair\n",
    "\n",
    "def load_run_data(subjID,runID,condname,filepath):\n",
    "    '''\n",
    "    Need to add description\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        varname: var type\n",
    "            Description\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        varname: var type\n",
    "            Description\n",
    "    '''\n",
    "    file1=os.path.join(filepath, ('{}_netLearn_{}_{}_410nodes.npz'.format(subjID,runID,condname)))\n",
    "    subj_runData=np.load(file1)['ppiMat']\n",
    "    subj_runData=convert_adj_matr_to_cfg_matr(np.expand_dims(subj_runData,axis=0))\n",
    "    return subj_runData\n",
    "    \n",
    "def merge_task_data(subjlist,condname,filepath):\n",
    "    '''\n",
    "    Need to add description\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        varname: var type\n",
    "            Description\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        varname: var type\n",
    "            Description\n",
    "    '''\n",
    "    #Create zero matrices to fill with subject data\n",
    "    combData=np.zeros((len(subjlist),n_conn))\n",
    "    meanDataNS=np.zeros((len(subjlist),n_conn))\n",
    "    meanDataSoc=np.zeros((len(subjlist),n_conn))\n",
    "    diffData=np.zeros((len(subjlist),n_conn))\n",
    "\n",
    "    for ii,subj in enumerate(subjlist):\n",
    "        if ii==0:\n",
    "            print('Working on subject #{}'.format(ii),end='')\n",
    "        elif ii%(math.ceil(len(subjlist)/10))==0:\n",
    "            print(ii,end='')\n",
    "        elif ii%(math.ceil(len(subjlist)/50))==0:\n",
    "            print('.'.format(ii),end='')\n",
    "        subj_run1Data=load_run_data(subj,'run1',condname,filepath)\n",
    "        subj_run2Data=load_run_data(subj,'run2',condname,filepath)\n",
    "        subj_run3Data=load_run_data(subj,'run3',condname,filepath)\n",
    "        subj_run4Data=load_run_data(subj,'run4',condname,filepath)\n",
    "        subj_run5Data=load_run_data(subj,'run5',condname,filepath)\n",
    "        subj_run6Data=load_run_data(subj,'run6',condname,filepath)\n",
    "        subj_run7Data=load_run_data(subj,'run7',condname,filepath)\n",
    "        subj_run8Data=load_run_data(subj,'run8',condname,filepath)\n",
    "        subj_run9Data=load_run_data(subj,'run9',condname,filepath)\n",
    "        subj_run10Data=load_run_data(subj,'run10',condname,filepath)\n",
    "        \n",
    "        #Average together the run data\n",
    "        subj_meanData1=(subj_run1Data+subj_run2Data+subj_run3Data+subj_run4Data+subj_run5Data)/5\n",
    "        subj_meanData2=(subj_run6Data+subj_run7Data+subj_run8Data+subj_run9Data+subj_run10Data)/5\n",
    "        subj_meanData3=(subj_meanData1+subj_meanData2)/2\n",
    "        \n",
    "        #Note: ppt in Condition #1 saw non-social task first, \n",
    "        #whereas ppt in Condition #2 saw social task first\n",
    "        if int(subj_links.loc[subj_links['scanID']==subj,'CondNum'])==1:\n",
    "            meanDataNS[ii,:]=subj_meanData1\n",
    "            meanDataSoc[ii,:]=subj_meanData2\n",
    "        elif int(subj_links.loc[subj_links['scanID']==subj,'CondNum'])==2:  \n",
    "            meanDataNS[ii,:]=subj_meanData2\n",
    "            meanDataSoc[ii,:]=subj_meanData1\n",
    "    \n",
    "        diffData[ii,:]=meanDataSoc[ii,:]-meanDataNS[ii,:]\n",
    "        combData[ii,:]=subj_meanData3\n",
    "        \n",
    "    #comb_cfg=convert_adj_matr_to_cfg_matr(combData)\n",
    "    #ns_cfg=convert_adj_matr_to_cfg_matr(meanDataNS)\n",
    "    #soc_cfg=convert_adj_matr_to_cfg_matr(meanDataSoc)\n",
    "    #diff_cfg=convert_adj_matr_to_cfg_matr(diffData)\n",
    "    saveFile1=os.path.join(filepath,'netLearn_comb_{}_{}subjs'.format(condname,len(subjlist)))\n",
    "    saveFile2=os.path.join(filepath,'netLearn_nonSoc_{}_{}subjs'.format(condname,len(subjlist)))\n",
    "    saveFile3=os.path.join(filepath,'netLearn_soc_{}_{}subjs'.format(condname,len(subjlist)))\n",
    "    saveFile4=os.path.join(filepath,'netLearn_diff_{}_{}subjs'.format(condname,len(subjlist)))\n",
    "    np.savez(saveFile1, ppiMat=combData, subjlist=subjlist)\n",
    "    np.savez(saveFile2, ppiMat=meanDataNS, subjlist=subjlist)\n",
    "    np.savez(saveFile3, ppiMat=meanDataSoc, subjlist=subjlist)\n",
    "    np.savez(saveFile4, ppiMat=diffData, subjlist=subjlist)\n",
    "    return combData,meanDataNS,meanDataSoc,diffData\n",
    "\n",
    "combData,meanDataNS,meanDataSoc,diffData=merge_task_data(subjs,'transition_PPI',path_OutpData4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7: Combine null models into single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ppiMat=matrix of ppi values for each node pair\n",
    "\n",
    "def load_null_data(subjID,runID,condname,filepath):\n",
    "    '''\n",
    "    Need to add description\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        varname: var type\n",
    "            Description\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        varname: var type\n",
    "            Description\n",
    "    '''\n",
    "    file1=os.path.join(filepath, ('{}_netLearn_{}_{}_410nodes_nulldist.npz'.format(subjID,runID,condname)))\n",
    "    subj_runData=np.load(file1)['ppiMat']\n",
    "    subj_runData=convert_adj_matr_to_cfg_matr(subj_runData)\n",
    "    return subj_runData\n",
    "\n",
    "#load_null_data(subjs[0],'run1','transition_PPI',path_NullData).shape\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "def merge_null_data(subjlist,condname,filepath):\n",
    "    '''\n",
    "    Need to add description\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        varname: var type\n",
    "            Description\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        varname: var type\n",
    "            Description\n",
    "    '''\n",
    "    #Create zero matrices to fill with subject data\n",
    "    combData=np.zeros((len(subjlist),n_perm,len(triu_ix)))\n",
    "    meanDataNS=np.zeros((len(subjlist),n_perm,len(triu_ix)))\n",
    "    meanDataSoc=np.zeros((len(subjlist),n_perm,len(triu_ix)))\n",
    "    diffData=np.zeros((len(subjlist),n_perm,len(triu_ix)))\n",
    "\n",
    "    for ii,subj in enumerate(subjlist):\n",
    "        if ii==0:\n",
    "            print('Working on subject #{}'.format(ii),end='')\n",
    "        elif ii%(math.ceil(len(subjlist)/10))==0:\n",
    "            print(ii,end='')\n",
    "        elif ii%(math.ceil(len(subjlist)/50))==0:\n",
    "            print('.'.format(ii),end='')\n",
    "        \n",
    "        subjData=np.zeros((len(runs),n_perm,n_conn))\n",
    "        for rr,run in enumerate(runs):\n",
    "            subjData[rr,:,:]=load_null_data(subj,runs[rr],condname,filepath)\n",
    "        \n",
    "        shuff_subjData=subjData.copy()\n",
    "        for pp in range(n_perm):\n",
    "            shuffle_runs=copy.copy(runs)\n",
    "            shuffle(shuffle_runs)\n",
    "            shuffle_runs=[int(x.replace('run','')) for x in shuffle_runs]\n",
    "            for x in range(len(runs)):\n",
    "                shuff_subjData[x,pp,:]=subjData[(shuffle_runs[x]-1),pp,:]\n",
    "            \n",
    "            #Average together the run data\n",
    "            subj_meanData1=np.mean(shuff_subjData[range(5),pp,:],axis=0)\n",
    "            subj_meanData2=np.mean(shuff_subjData[range(5,10),pp,:],axis=0)\n",
    "            subj_meanData3=np.mean(shuff_subjData[:,pp,:],axis=0)\n",
    "            \n",
    "            meanDataNS[ii,pp,:]=subj_meanData1\n",
    "            meanDataSoc[ii,pp,:]=subj_meanData2\n",
    "            \n",
    "            diffData[ii,pp,:]=subj_meanData1-subj_meanData2\n",
    "            combData[ii,pp,:]=subj_meanData3\n",
    "    \n",
    "    saveFile1=os.path.join(filepath,'netLearn_comb_nulldata_{}_{}subjs'.format(condname,len(subjlist)))\n",
    "    saveFile2=os.path.join(filepath,'netLearn_nonSoc_nulldata_{}_{}subjs'.format(condname,len(subjlist)))\n",
    "    saveFile3=os.path.join(filepath,'netLearn_soc_nulldata_{}_{}subjs'.format(condname,len(subjlist)))\n",
    "    saveFile4=os.path.join(filepath,'netLearn_diff_nulldata_{}_{}subjs'.format(condname,len(subjlist)))\n",
    "    np.savez(saveFile1, cfgMat=combData, subjlist=subjlist)\n",
    "    np.savez(saveFile2, cfgMat=meanDataNS, subjlist=subjlist)\n",
    "    np.savez(saveFile3, cfgMat=meanDataSoc, subjlist=subjlist)\n",
    "    np.savez(saveFile4, cfgMat=diffData, subjlist=subjlist)\n",
    "    print('')\n",
    "    print('Saved file {}'.format(saveFile1))\n",
    "    print('Saved file {}'.format(saveFile2))\n",
    "    print('Saved file {}'.format(saveFile3))\n",
    "    print('Saved file {}'.format(saveFile4))\n",
    "\n",
    "    return combData,meanDataNS,meanDataSoc,diffData\n",
    "\n",
    "null_combData,null_meanDataNS,null_meanDataSoc,null_diffData=merge_null_data(subjs,'transition_PPI',path_OutpData5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steven Tompson | 2019"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
