{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GLM Z-Score Analysis\n",
    "=====================================\n",
    "\n",
    "This script gets z-scores and p-values for the Schaefer atlas nodes by\n",
    "\n",
    "1.  loading the second-level GLM z-maps for each contrast, \n",
    "2.  calculating the average value for each node in the Schaefer atlas, \n",
    "3.  computing the z-score and p-values for each node in the Schaefer atlas, and saving the z-scores and p-values into a new npz file.\n",
    "\n",
    "# Get hub activation for social and non-social tasks\n",
    "\n",
    "1. Load subject info (cond order list) and atlas info\n",
    "2. Get dict of hubs with ROI indices\n",
    "3. Create mask for each set of hubs that you want to extract average activation from\n",
    "4. Load transition versus non-transition nii file for each task\n",
    "5. Extract average activation in mask for each set of hubs\n",
    "6. Loop over each subject & task and fill empty pandas dataframe\n",
    "7. Save pandas df as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "%matplotlib inline\n",
    "from __future__ import print_function, division\n",
    "import scipy.stats as ss\n",
    "import os\n",
    "from os.path import join as opj\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import inf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nilearn import image, plotting, input_data\n",
    "import nibabel as nb\n",
    "from mne.stats import fdr_correction\n",
    "from nistats import thresholding\n",
    "from random import shuffle\n",
    "import glob\n",
    "\n",
    "print('Done Importing Packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to directory where you saved the data\n",
    "home_path = '/Users/steventompson/Git/tompson_netlearn_fmri'\n",
    "\n",
    "data_dir = opj(home_path,'data')\n",
    "template_dir = opj(data_dir,'brain_atlas')\n",
    "path_InpData = opj(data_dir,'Subject_Data','netLearn_glm','firstLevel')\n",
    "path_zData = opj(data_dir,'ppi_zscores')\n",
    "path_OutpData = opj(data_dir,'glm_means')\n",
    "path_Figures = opj(home_path,'figures','component_figs') # folder to put figures\n",
    "\n",
    "\n",
    "for path in [path_OutpData, path_Figures]:\n",
    "    if not os.path.exists(path):\n",
    "        print('Path does not exist, creating {}'.format(path))\n",
    "        os.makedirs(path)\n",
    "\n",
    "print('Set data paths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_links=pd.read_csv('{}/subj_data/netLearn_IDs_26subjs.csv'.format(data_dir))\n",
    "\n",
    "bad_subjs = ['SNL_001','SNL_004','SNL_028']\n",
    "subjs = [s for s in subj_links.loc[:,'scanID'].tolist() if s not in bad_subjs]\n",
    "#subjs.reverse()\n",
    "n_subjs = len(subjs)\n",
    "print('We have %d subjects' % (n_subjs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load schaefer mask\n",
    "sch_filename='{}/schaefer400_harvard_oxford_2mm_mni_17network.nii.gz'.format(template_dir)\n",
    "schaefer_mask=nb.load(sch_filename)\n",
    "\n",
    "#load schaefer atlas info\n",
    "schaefer_atlas=pd.read_csv('{}/s400ho_netLearn_2mm.csv'.format(template_dir))\n",
    "\n",
    "schaefer_atlas=schaefer_atlas.fillna('Uncertain')\n",
    "schaefer_atlas.loc[schaefer_atlas['System']=='Uncertain','System']='Subcortical'\n",
    "\n",
    "schaefer_atlas.loc[[402,407],'System']='Hippocampus'\n",
    "schaefer_atlas.loc[[402,407],'System7']='Hippocampus'\n",
    "\n",
    "sch_names=np.unique(schaefer_atlas['System'])\n",
    "sch_nums=[int(np.where(sch_names==label)[0]) for label in schaefer_atlas['System']]\n",
    "\n",
    "\n",
    "schaefer_atlas.loc[schaefer_atlas['System7']=='Uncertain','System7']='Subcortical'\n",
    "sch7_names=np.unique(schaefer_atlas['System7'])\n",
    "sch7_nums=[int(np.where(sch7_names==label)[0]) for label in schaefer_atlas['System7']]\n",
    "\n",
    "net_coords=np.array(schaefer_atlas.loc[:,['x','y','z']])\n",
    "net_cols=['black']*len(net_coords)\n",
    "\n",
    "n_node = len(sch_nums)\n",
    "triu_ix, triu_iy = np.triu_indices(n_node, k=1)\n",
    "n_conn = len(triu_ix)\n",
    "\n",
    "n_perm = 500\n",
    "\n",
    "schaefer_atlas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_links.loc[:,'nsFile']='NA'\n",
    "subj_links.loc[:,'socFile']='NA'\n",
    "for subj in subj_links['pID']:\n",
    "    cond=subj_links.loc[subj_links['pID']==subj,'CondNum'].tolist()[0]\n",
    "    scanID=subj_links.loc[subj_links['pID']==subj,'scanID'].tolist()[0]\n",
    "    if cond==1:\n",
    "        nsFile='{}_task1_Transition_z_map.nii.gz'.format(scanID)\n",
    "        socFile='{}_task2_Transition_z_map.nii.gz'.format(scanID)\n",
    "    else:\n",
    "        nsFile='{}_task2_Transition_z_map.nii.gz'.format(scanID)\n",
    "        socFile='{}_task1_Transition_z_map.nii.gz'.format(scanID)\n",
    "    subj_links.loc[subj_links['pID']==subj,'nsFile']=nsFile\n",
    "    subj_links.loc[subj_links['pID']==subj,'socFile']=socFile\n",
    "    \n",
    "ns_filenames=subj_links['nsFile'].values\n",
    "soc_filenames=subj_links['socFile'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "list1=['combData','diffData','nonSocData','SocData']\n",
    "list1_labels=['All Tasks','Social versus Non-Social','Non-Social Task','Social Task']\n",
    "list2=['nodestr']\n",
    "list2_labels=['Global']\n",
    "\n",
    "\n",
    "apriori_names=['dmPFC_L','vmPFC_L','PCC_L','Hippocampus_L','TPJ_L',\n",
    "               'dmPFC_R','vmPFC_R','PCC_R','Hippocampus_R','TPJ_R',\n",
    "               'Frontal_Inf_Oper_L','Frontal_Inf_Orb_L','Frontal_Inf_Tri_L',\n",
    "               'Frontal_Inf_Oper_R','Frontal_Inf_Orb_R','Frontal_Inf_Tri_R',\n",
    "               'Amygdala_L','Amygdala_R',\n",
    "               'Ventral_Striatum_L','Caudate_L',\n",
    "               'Ventral_Striatum_R','Caudate_R']\n",
    "\n",
    "\n",
    "\n",
    "def identify_hubs1(ix,iy,thresh=True,apriori=True,flip=False,alpha=0.05):\n",
    "    xx=list2[ix]\n",
    "    yy=list1[iy]\n",
    "    data=np.load('{}/netLearn_{}_zscores_26subjs_{}.npz'.format(path_zData,yy,xx))\n",
    "    zMat=data['zMat']\n",
    "    pMat=data['pMat']\n",
    "    #pMat=np.multiply(data['pMat'],2)\n",
    "    \n",
    "    if flip:\n",
    "        zMat=np.multiply(zMat,-1)\n",
    "    \n",
    "    # Set nonsignificant values to zero using FDR correction\n",
    "    if thresh:\n",
    "        reject_fdr,pval_fdr=fdr_correction(pMat,alpha)\n",
    "        zMat=np.multiply(zMat,reject_fdr)\n",
    "\n",
    "    title='{}- {} Connectivity'.format(list1_labels[iy],list2_labels[ix])\n",
    "    sig_ix=np.where(zMat>0)[0]\n",
    "    sig_dic={}\n",
    "    sig_vec=np.zeros(zMat.shape)\n",
    "    if apriori:\n",
    "        sig_ix=[x for x in sig_ix if schaefer_atlas.loc[x,'ns_ROI_label'] in apriori_names]\n",
    "        for i,x in enumerate(apriori_names):\n",
    "            sig_dic[x]=[ix for ix in sig_ix if schaefer_atlas.loc[ix,'ns_ROI_label']==x]\n",
    "            sig_dic=dict((k, v) for k, v in sig_dic.iteritems() if v)\n",
    "\n",
    "    sig_vec[sig_ix]=1\n",
    "        \n",
    "    print('')\n",
    "    print(title)\n",
    "    print('{} significant positive ROIs in a priori regions'.format(len(sig_ix)))\n",
    "    return sig_ix,sig_vec,sig_dic\n",
    "    \n",
    "sigvals_comb,sigvec_comb,sigdic_comb=identify_hubs1(0,0,alpha=0.025)\n",
    "signames_comb=schaefer_atlas.loc[sigvals_comb,'ns_ROI_label']\n",
    "\n",
    "sigvals_diff,sigvec_diff,sigdic_diff=identify_hubs1(0,1,alpha=0.025)\n",
    "signames_diff=schaefer_atlas.loc[sigvals_diff,'ns_ROI_label']\n",
    "\n",
    "\n",
    "sigvals_diff_ns,sigvec_diff_ns,sigdic_diff_ns=identify_hubs1(0,1,flip=True,alpha=0.025)\n",
    "signames_diff_ns=schaefer_atlas.loc[sigvals_diff_ns,'ns_ROI_label']\n",
    "\n",
    "sigdic_comb['Hippocampus']=[402,407]\n",
    "sigdic_comb['combhubs']=sigvals_comb\n",
    "sigdic_diff['diffhubs']=sigvals_diff\n",
    "sigdic_diff_ns['diffhubs_ns']=sigvals_diff_ns\n",
    "\n",
    "lpfc_l_vals=[sigdic_diff_ns[key] for key in sigdic_diff_ns.keys() if 'Frontal' in key and '_L' in key]\n",
    "sigdic_diff_ns['lPFC_L']=[item for sublist in lpfc_l_vals for item in sublist]\n",
    "lpfc_r_vals=[sigdic_diff_ns[key] for key in sigdic_diff_ns.keys() if 'Frontal' in key and '_R' in key]\n",
    "sigdic_diff_ns['lPFC_R']=[item for sublist in lpfc_r_vals for item in sublist]\n",
    "\n",
    "\n",
    "for x in sigdic_comb.keys():\n",
    "    print('DG Hubs: Column ids for {}= {}'.format(x,sigdic_comb[x]))\n",
    "    \n",
    "print('')\n",
    "for x in sigdic_diff.keys():\n",
    "    print('Soc Hubs: Column ids for {}= {}'.format(x,sigdic_diff[x]))\n",
    "    \n",
    "print('')\n",
    "for x in sigdic_diff_ns.keys():\n",
    "    print('NS Hubs: Column ids for {}= {}'.format(x,sigdic_diff_ns[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_atlas_means(flist,fpath,atlas_path):\n",
    "    fnames=[opj(fpath,f) for f in flist]\n",
    "    print('Concatenating {} images'.format(len(fnames)))\n",
    "    all_imgs=image.concat_imgs(fnames,memory='nilearn_cache',memory_level=1)\n",
    "    masker = input_data.NiftiLabelsMasker(atlas_path,\n",
    "                                           detrend=False, \n",
    "                                           standardize=False, \n",
    "                                           low_pass=None, high_pass=None, \n",
    "                                           t_r=1,\n",
    "                                           memory='nilearn_cache', memory_level=1); #verbose=2 by default nothing should be printed\n",
    "\n",
    "    print('Extracting values from nii image')\n",
    "    all_ts = masker.fit_transform(all_imgs).squeeze()\n",
    "\n",
    "    print('Matrix created with shape {}'.format(all_ts.shape))\n",
    "    return all_ts\n",
    "\n",
    "# In[ ]:\n",
    "#adj_mat is 2-d matrix with n_subj x n_node shape\n",
    "#row_ix is a list of indices to keep for the rows\n",
    "#col_ix is a list of indices to keep for the columns\n",
    "def get_means(roi_mat,row_ix,col_ix):\n",
    "    #cfg_mat is a 3D n_subjs x n_perm x n_conn np array\n",
    "    #row_ix is a list of the row indices to include in the means to extract\n",
    "    #col_ix is a list of the column indices to include in the means to extract\n",
    "    subdata=roi_mat[row_ix,:]\n",
    "    subdata=subdata[:,col_ix]\n",
    "    rowMeans=np.mean(subdata,axis=(-1))\n",
    "    return rowMeans\n",
    "\n",
    "def get_subhub_means(roi_mat,col_dict,col_id):\n",
    "    #roi_cfg is a 2D n_subjs x n_conn np array\n",
    "    #nulll_cfg is a 3D n_subjs x n_perm x n_conn np array\n",
    "    #row_id is a string that matches a key in the row_dict (or is 'all' which will get global connectivity)\n",
    "    #row_dict is a dictionary with keys matching a priori hubs and row indices for each hub\n",
    "    #col_id is a string that matches a key in the col_dict (or is 'all' which will get global connectivity)\n",
    "    #col_dict is a dictionary with keys matching a priori hubs and column indices for each hub\n",
    "    #thresh is a boolean determining whether to set non-significant z-scores to zero \n",
    "    if col_id=='all':\n",
    "        col_ix=range(410)\n",
    "    else:\n",
    "        col_ix=col_dict[col_id]\n",
    "        \n",
    "    print('Getting subject averages for {}'.format(col_id))\n",
    "    subj_vec=get_means(roi_mat,range(roi_mat.shape[0]),col_ix)\n",
    "    return subj_vec\n",
    "    \n",
    "def subj_activation(file_list,file_path,atlas_file,savename,savepath):\n",
    "\n",
    "    # load activation maps and get mean activation for each node in atlas\n",
    "    node_means=get_atlas_means(file_list,file_path,atlas_file)\n",
    "    \n",
    "    #df=pd.DataFrame({'pID':[f[:7] for f in file_list]})\n",
    "    df=pd.DataFrame()\n",
    "    \n",
    "    for a in ['combhubs','Hippocampus_L','Hippocampus_R']:\n",
    "    #for a in ['Hippocampus','Hippocampus_L','Hippocampus_R','Frontal_Inf_Orb_L','Frontal_Inf_Tri_R','dmPFC_L','dmPFC_R','vmPFC_L']:\n",
    "            colname='{}_glm'.format(a)\n",
    "            sub_means=get_subhub_means(node_means,sigdic_comb,a)\n",
    "            df[colname]=sub_means\n",
    "\n",
    "    for a in ['diffhubs','TPJ_L','TPJ_R']:\n",
    "            colname='{}_glm'.format(a)\n",
    "            sub_means=get_subhub_means(node_means,sigdic_diff,a)\n",
    "            df[colname]=sub_means\n",
    "            \n",
    "    for a in ['diffhubs_ns','dmPFC_L','dmPFC_R','lPFC_L','lPFC_R']:\n",
    "            colname='{}_glm'.format(a)\n",
    "            sub_means=get_subhub_means(node_means,sigdic_diff_ns,a)\n",
    "            df[colname]=sub_means\n",
    "     \n",
    "    save_file='{}/netLearn_{}_26subjs_sighubs_activation.csv'.format(savepath,savename)\n",
    "    df.to_csv(save_file)\n",
    "    print('saving df with shape {} to file: {}'.format(df.shape,opj(save_file)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_df=subj_activation(file_list=ns_filenames,file_path=path_InpData,atlas_file=sch_filename,savename='nsData_glm_transition',savepath=path_OutpData)\n",
    "soc_df=subj_activation(file_list=soc_filenames,file_path=path_InpData,atlas_file=sch_filename,savename='socData_glm_transition',savepath=path_OutpData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soc_df.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steven Tompson | 2019"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
